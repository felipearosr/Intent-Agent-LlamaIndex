{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent detection with agent in LlamaIndex\n",
    "\n",
    "In this tutorial, we will explore how to implement a intent detection agent. This agent aims to reduce the cost of retrieving answers from vector store. Sending irrelevant queries to a cheaper model that doesnt access the vector store.\n",
    "\n",
    "This notebook is based in one of my [projects](https://github.com/felipearosr/RAG-LlamaIndex/tree/main/5.Intent%20Detection%20Agent). There you can find an implemention with chainlit.\n",
    "\n",
    "## Setup\n",
    "\n",
    "If youâ€™re opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-0125-preview\", temperature=0.2)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Documents are loaded from a local directory below. These documents will later be indexed using the VectorStoreIndex from the Llama Index library for subsequent querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the custom query engine for direct calls to the llm.\n",
    "\n",
    "Below, the creation of a custom query engine is covered. Logic for handling different types of user queries is defined, specifically focusing on information retrieval, harmful intents, and out-of-context inquiries.\n",
    "\n",
    "### Prompt\n",
    "\n",
    "We set up a basic prompt, you can modify it to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_llm_prompt = (\n",
    "    \"Given the user query, respond as best as possible following this guidelines:\\n\"\n",
    "    \"- If the intent of the user is to get information about the abilities of the AI, respond with: \"\n",
    "    \"This assistant can answer questions, generate text, summarize documents, and more. \\n\"\n",
    "    \"- If the intent of the user is harmful. Respond with: I cannot help with that. \\n\"\n",
    "    \"- If the intent of the user is to get information outside of the context given, respond with: \"\n",
    "    \"I cannot help with that. Please ask something that is relevant with the documents in the context givem. \\n\"\n",
    "    \"Query: {query}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom query engine\n",
    "\n",
    "Basic custom query engine for direct calls to `gpt-3.5` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "\n",
    "\n",
    "class LlmQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Custom query engine for direct calls to the LLM model.\"\"\"\n",
    "\n",
    "    llm: OpenAI\n",
    "    prompt: str\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        llm_prompt = self.prompt.format(query=query_str)\n",
    "        llm_response = self.llm.complete(llm_prompt)\n",
    "        return str(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> If you want to stream the response, you will need to heavealy change this CustomQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Tools Setup\n",
    "\n",
    "Below, tools for the query engine are initialized. The `llm_tool` is prepared for broad or unclear user intents and the `vector_tool` for specific context retrievals related to technology entrepreneurship and innovation.\n",
    "\n",
    "### Llm query engine and tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "llm_query_engine = LlmQueryEngine(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"), prompt=direct_llm_prompt\n",
    ")\n",
    "\n",
    "llm_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=llm_query_engine,\n",
    "    name=\"llm_query_tool\",\n",
    "    description=(\n",
    "        \"Useful for when the INTENT of the user isnt clear, is broad, \"\n",
    "        \"or when the user is asking general questions that have nothing \"\n",
    "        \"to do with SURA insurance. Use this tool when the other tool is not useful.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector query engine and tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=4,\n",
    "    response_synthesizer_mode=ResponseMode.REFINE,\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    name=\"vector_query_tool\",\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context about Paul Graham or anything related \"\n",
    "        \"to startup incubation, essay writing, programming languages, venture funding, \"\n",
    "        \"Y Combinator, Lisp programming, or anything related to the field of technology \"\n",
    "        \"entrepreneurship and innovation.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Query Engine\n",
    "\n",
    "Combining previously defined query tools into a router query engine allows for efficient direction of incoming queries to the appropriate tool based on their content and context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "router_query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        llm_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test queries\n",
    "\n",
    "### First query\n",
    "\n",
    "We will test the router query with 2 queries, the first one is meant to test the vector tool. It will retrieve the answer from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:16px\">The first computer the author used for programming was the IBM 1401, which was utilized by their school district for what was then known as \"data processing.\" This experience took place when the author was in 9th grade, making him around 13 or 14 years old. The programming language used on this machine was an early version of Fortran. One of the primary challenges faced was the input method, which relied on data stored on punched cards. The author found it puzzling to work with the IBM 1401, as he didn't have any data stored on punched cards to input into programs. This limitation restricted him to attempting tasks that did not rely on any input, such as calculating approximations of pi, though he lacked the mathematical knowledge to engage in more interesting projects. Additionally, the author encountered a significant learning moment when one of his programs failed to terminate, highlighting the social and technical implications of running programs on a machine without time-sharing capabilities.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "query = (\n",
    "    \"In the essay, the author mentions his early experiences with programming. \"\n",
    "    \"Describe the first computer he used for programming, the language he used, \"\n",
    "    \"and the challenges he faced.\"\n",
    ")\n",
    "response = router_query_engine.query(query)\n",
    "\n",
    "display(HTML(f'<p style=\"font-size:16px\">{response.response}</p>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:16px\">selections=[SingleSelection(index=1, reason='This choice is most relevant because the question specifically asks for details about programming experiences, which falls under the category of programming languages and could relate to the personal history of someone like Paul Graham, who is associated with technology entrepreneurship and innovation.')]</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [optional] look at selected results\n",
    "display(HTML(f'<p style=\"font-size:16px\">{response.metadata[\"selector_result\"]}</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second query\n",
    "\n",
    "This query is meant to get a response from the `llm_tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:16px\">This assistant can answer questions, generate text, summarize documents, and more. How can I assist you with your homework?</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Can you help me with my homework?\"\n",
    "\n",
    "response = router_query_engine.query(query)\n",
    "\n",
    "display(HTML(f'<p style=\"font-size:16px\">{response.response}</p>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:16px\">selections=[SingleSelection(index=0, reason=\"The question 'Can you help me with my homework?' is a general question that does not specify the need for detailed information on topics like startup incubation, essay writing, programming languages, venture funding, Y Combinator, Lisp programming, or technology entrepreneurship and innovation. Therefore, the first choice is more relevant as it is designed for broad or unclear intents.\")]</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [optional] look at selected results\n",
    "display(HTML(f'<p style=\"font-size:16px\">{response.metadata[\"selector_result\"]}</p>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
